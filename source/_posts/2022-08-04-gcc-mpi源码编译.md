---
title: gcc mpi源码编译
date: 2022-08-04 16:28:41
tags:
- gcc
- mpi
categories:
---

gcc mpi源码编译过程记录
<!-- more -->

### GCC

#### 下载源码

- 官方[GNU Mirror List - GNU Project - Free Software Foundation](https://www.gnu.org/prep/ftp.html)
  - https://ftp.gnu.org/gnu/
  - http://ftp.gnu.org/gnu/

- 国内镜像
  - https://mirrors.aliyun.com/gnu/
  - https://mirrors.ustc.edu.cn/gnu/
  - https://mirrors.tuna.tsinghua.edu.cn/gnu/

#### Configure

注意：

- 不能在src目录configure，而是应该另外创建一个build目录
- gcc提供了依赖安装脚本，不用手动去下载

```bash
cd /tmp/dir
wget https://mirrors.ustc.edu.cn/gnu/gcc/gcc-8.5.0/gcc-8.5.0.tar.xz
tar xf gcc-8.5.0.tar.xz
cd gcc-8.5.0
#安装依赖
./contrib/download_prerequisites

#在build目录下configure
cd ..
mkdir build
cd build

../gcc-8.5.0/configure --disable-multilib --prefix=$HOME/ENV/gcc-8.5.0 --enable-languages=c,c++

make -j 2>&1 |tee make.log	#32线程编译大概40分钟左右
make install
```

- --disable-multilib：gcc默认支持编译32位程序(-m32)，关闭此功能

#### 解决链接库错误

原因，程序是动态链接的。运行时使用的是系统的库。而系统的库和编译时使用的库不兼容。

- 方法一：修改`LD_LIBRARY_PATH`环境变量

  ```
  export LD_LIBRARY_PATH=$HOME/opt/gcc-9.1.0/lib64
  ```

- 方法二：编译时将库搜索路径存在二进制文件中

  ```
  ~/opt/gcc-9.1.0/bin/g++ -Wl,-rpath=$HOME/opt/gcc-9.1.0/lib64 --std=c++17 demo.cc -o demo
  ```

- 方法三：静态链接

  ```
  ~/opt/gcc-9.1.0/bin/g++ --std=c++17 demo.cc -o demo -static-libstdc++ -static-libgcc
  ```

#### 遇到的问题

##### 解压源码很慢

服务器上解压60M的源码非常慢，解压了10-20分钟。不只是解压，cp, rm源码目录都非常的慢。推测是NFS的原因。

解决方法为使用本地目录，如/tmp

##### gcc8.3.0 make失败

[Building gcc 8.3 [Makefile:955: all\] Error 2 - Stack Overflow](https://stackoverflow.com/questions/62435946/building-gcc-8-3-makefile955-all-error-2)

好像是8.3.0自身的问题，换用8.5.0编译成功

### MPI

MPI编译较快，可以编译很多版本使用

#### mpich和openmpi对比

MPICH：[Guides | MPICH](https://www.mpich.org/documentation/guides/)

- 支持mpi2.0，功能强大，效率高
- 缺点是仅支持以太网

OpenMPI：

- 支持mpi2.0

- 对于CPU核心较多的节点，推荐使用 openmpi
- 支持各种网络，包括以太网和infiniband

#### mpich

- 参考官方文档：[mpich-4.0.2-installguide.pdf](https://www.mpich.org/static/downloads/4.0.2/mpich-4.0.2-installguide.pdf)

```
../configure -prefix=$HOME/ENV/mpich-4.0.2-static CC=gcc CXX=g++ --disable-fortran --enable-fast=all,O3 --with-ucx=embedded --disable-shared --enable-static
```

- --disable-fortran

  - 文档里写的是--disable-f77：禁用fortran 77，--disable-fc：禁用Fortran 90（及之后95, 2003, 2008版本）但是尝试后无效，仍然会要求你有fortran编译器。

- --enable-fast

  MPICH libraries are built with default compiler optimization, -O2, which can be modified by --enable-fast configure option. For instance, --disable-fast disables the default optimization option. --enable-fast=O sets default compiler optimization as -O<n>

- --enable-shared
  - mpicc编译出来的程序使用动态链接，运行时需要设置`LD_LIBRARY_PATH`。
  - --disable-shared --enable-static则不用设置`LD_LIBRARY_PATH`

##### Process Manger

- --with-pm=hydra

MPICH has been designed to work with multiple process managers; that is, although you can start MPICH jobs with mpiexec, there are different mechanisms by which your processes are started. An interface (called PMI) isolates the MPICH library code from the process manager. Currently three process managers are distributed with MPICH

- hydra This is the default process manager that natively uses the existing daemons on the system such as ssh, slurm, pbs.
- gforker This is a simple process manager that creates all processes on a single machine. It is useful both for debugging and for running on shared memory multiprocessors.

##### 遇到的问题

######  UCX version

configure: error: UCX installation does not meet minimum version requirement (v1.9.0). Please upgrade your installation, or use --with-ucx=embedded

###### mpirun多节点卡住

换用--disable-shared --enable-static编译，结果编译错误

```
/usr/bin/ld: /tmp/yfy/build-mpi/modules/ucx/src/ucm/.libs/libucm.a(libucm_la-event.o): in function `ucm_event_install':
/tmp/yfy/build-mpi/modules/ucx/src/ucm/../../../../../mpich-4.0.2/modules/ucx/src/ucm/event/event.c:536: undefined reference to `ucs_load_modules'
collect2: error: ld returned 1 exit status
```

#### openmpi

- [4.7. configure command line options — Open MPI 5.0.x documentation (open-mpi.org)](https://docs.open-mpi.org/en/v5.0.x/installing-open-mpi/configure-cli-options/index.html)

```
../openmpi-4.1.4/configure --prefix=$HOME/ENV/openmpi-4.1.4 --disable-mpi-fortran CC=gcc CXX=g++
```

##### 运行选项

```
-v	verbose
-q	quiet
--use-hwthread-cpus		#使用物理核作为slot数目

-H, -host, --host <host1,host2,...,hostN>
-hostfile, --hostfile <hostfile>
    % cat myhostfile
    aa slots=2
    bb slots=2
    cc slots=2

-c, -n, --n, -np <#>
-cpus-per-proc, --cpus-per-proc <#perproc>
-bind-to-core, --bind-to-core
-bind-to-socket, --bind-to-socket
```

##### 遇到的问题

###### run

```
➜  workspace mpirun -n 4 ./mpi_hello
--------------------------------------------------------------------------
By default, for Open MPI 4.0 and later, infiniband ports on a device
are not used by default.  The intent is to use UCX for these devices.
You can override this policy by setting the btl_openib_allow_ib MCA parameter
to true.

  Local host:              snode0
  Local adapter:           mlx5_0
  Local port:              1

--------------------------------------------------------------------------
--------------------------------------------------------------------------
WARNING: There was an error initializing an OpenFabrics device.

  Local host:   snode0
  Local device: mlx5_0
--------------------------------------------------------------------------
Hello world from processor snode0, rank 3 out of 4 processors
Hello world from processor snode0, rank 0 out of 4 processors
Hello world from processor snode0, rank 1 out of 4 processors
Hello world from processor snode0, rank 2 out of 4 processors
[snode0:2773927] 3 more processes have sent help message help-mpi-btl-openib.txt / ib port not selected
[snode0:2773927] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[snode0:2773927] 3 more processes have sent help message help-mpi-btl-openib.txt / error in device init
```

