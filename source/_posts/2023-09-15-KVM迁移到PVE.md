---
title: 2023-09-15-KVM迁移到PVE
date: 2023-09-15 13:08
tags:
  - linux
  - pve
  - KVM
  - 虚拟化
categories:
  - homelab
  - 虚拟化
---
## 背景

之前换1070 虚拟机老是蓝屏，重装虚拟C盘后，稳定了一段时间。

结果09-12晚，尝试在host中装下nvidia驱动（想发挥剩余1063的作用），然后重启了host。结果就访问不上了，第二天去实验室发现，网络出现了很神奇的问题
- 能够ping通，但是就是无法打开网页。curl -v会卡住

下午尝试恢复无果，09-13晚windows居然又蓝屏了。

于是Plan C启动，下定决心将host迁移到PVE上了，连续搞了2天加一晚上

<!-- more -->
### 前传：换1070导致蓝屏不稳定

因为想玩博德之门3，换了张1070，快递2天到了(09-08)

硬件安装
- 6 pin供电。刚开始直接复用了现有的线（一根线根据长度有两个头），后面发现电源一个接口可能不支持复用。于是增加了一根线。
- 1070长度很长，如果装第二条pciex16的槽的话，会盖住3个sata接口，并且sata接口必须是扁头才能装上。（大力出奇迹把一个sata接口压断了，很脆弱）

装好后启动，会有屏幕闪烁线
- 好像是dp线和hdmi线松动导致的。手拨动线时，屏幕闪烁也跟着变化，调整几下，屏幕就正常了

进入windows后刚开始一切正常，任务管理器是正确的1070，结果运行了10几分钟，kvm-win10突然蓝屏重启了。以为是小问题，因为后面测试一段时间都没遇到。

3dmark无法运行（collect systeminfo卡住）。尝试鲁大师跑分，只有21万，感觉不到1063的1.5倍

甜甜圈烤鸡，结果发现不到5分钟就蓝屏
- gpu temperature: 86
- hot spot: 96
- board power draw: 130W左右

之后发现甜甜圈烤鸡能稳定触发蓝屏，询问卖家，说是温度高了（说换硅脂能降到70度），让我装了微星小飞机，小飞机自带温度墙功能，默认设置为83度。之后烤鸡确实不会蓝屏了，温度会被限制在83度

**然后噩梦就开始了：**
- 刚开始鲁大师固定触发蓝屏，然后意识到之前修改过qemu配置文件中的hyperv选项。修改回去后，鲁大师打开不蓝屏了
- 以为好了，于是玩尼尔机械纪元开高画质逛了几下，结果不久蓝屏
    - 后面将温度控制在77以下，核心频率-100MHz，风扇80，结果还是会稳定触发蓝屏，每次最长10分钟就蓝屏
- 后面甚至windows重启后不操作，结果在登录界面蓝屏了
- 期间尝试了重装gpu驱动，还试了旧版本
蓝屏报的错误是VIDEO_DXGKRNL_FATAL_ERROR，搜索不出结果

换回1063，结果仍然蓝屏，并且nvidia geforece experience还装不上。

3个计划：
1）重装虚拟机C盘。
- 换回1063 + rx550，结果也会遇到蓝屏，因此问题确定是虚拟机系统的问题。
	- 表现一：之前遇到nvidia gforece experience打不开，报003错误。重装gfe不能解决问题。
	- 表现二：修改了qemu配置文件后，运行鲁大师很可能直接蓝屏
	- 表现三：玩游戏如尼尔机械纪元，不定时蓝屏，试了很多次5-10分钟就会遇到蓝屏。
	- 表现四：不运行游戏，也会蓝屏。在复制H盘数据到F盘的过程中，遇到了好几次

重装了虚拟机C盘后，一晚上暂时没遇到过蓝屏现象

2）测试裸机
- 测试是否蓝屏
- 测试博德之门是否可以运行
由于kvm虚拟机的磁盘和host的磁盘无法直接共享，因此没有游戏数据。再加上装在机械盘上的win10实在太慢，因此计划作罢。

3）PVE迁移

理由
- 不清楚本次蓝屏问题的根本原因，担心下次还会遇到。而PVE虚拟化支持应该更完善，遇到这种问题概率更小
- 双win10虚拟机，那么现在的1060 + 1070都能发挥作用了
- 管理虚拟机更加整洁。现在混用了多种磁盘格式，并且分布在不同磁盘上。

## 工作量

- lxc容器
    - op1软路由
      - lxc image export的压缩包内包含rootfs，pve可以将其作为CT template，直接启动
    - ubuntu20 jellyfin server
    - ubuntu22 warp trojan-go
- KVM虚拟机
    - C盘重装
    - D盘，E盘(raw)：qm importdisk
    - G,H: zfs rename
    - F盘：直通分区
- linux host
    - root：重装
    - home：rsync
    - docker
        - nfs挂载PT盘
## LXC容器

pve里的ct template其实就是rootfs，而lxc image export得到的tar.gz的压缩包，解压便可以得到meta.yaml和rootfs目录，我们只需要其中rootfs部分，压缩上传到pve的template，然后通过该模板创建新容器即可

- tar压缩时不包含顶层目录
```
tar czf xxx.tar.gz -C <dir> .
```

- ubuntu22容器没有备份，不过zfs对应的dataset还存在。不过无法直接访问dataset对应路径。解决办法是手动mount dataset
```
mount -t zfs lxd/containers/mycontainer2 /xxx/mycontainer2
```

## KVM虚拟机

C,D,E原本为raw文件
通过qm将其导入，由于是zfs，会创建成subvol
```
qm disk import 200 disk-common.raw local-zfs
```

至于原本就是zfs zvol的盘，直接zfs rename pve对应的磁盘格式即可识别。
```
➜  modprobe.d pvesm list pool0
Volid                       Format  Type              Size VMID
pool0:vm-101-disk-docker    raw     images    137438953472 101
pool0:vm-101-disk-home      raw     images    107374182400 101
pool0:vm-200-disk-gamessd   raw     images    644245094400 200
pool0:vm-200-disk-simulator raw     images    644245094400 200
```

对于原本的F盘，需要直通磁盘分区。这个pve里也很容易
```
qm set 200 -scsi5 /dev/disk/by-id/wwn-0x5000c500dca9e8c1-part2
```
### 系统盘

尝试直接从原本系统盘启动，但是报`INACCESSIBLE BOOT DEVICE`错误
![image.png](https://raw.githubusercontent.com/TheRainstorm/.image-bed/main/20230914123749.png)

尝试使用微PE修复引导。但是微PE内看不到磁盘。

最后只好重装系统盘（这次选择了omvf固件），重装时是可以看到原系统盘的三个分区的。
![image.png](https://raw.githubusercontent.com/TheRainstorm/.image-bed/main/20230914123124.png)

通过尝试安装到旧盘上，可以发现
- 旧盘使用mbr分区表
- 旧盘没有MSR分区（原本是通过seabios启动，所以可能MSR位于磁盘第一个扇区？）
![image.png](https://raw.githubusercontent.com/TheRainstorm/.image-bed/main/20230914130027.png)

## Linux host

### 通过nfs在host和vm间共享数据

由于数据都在pve host里面，那么如何在vm访问呢？比如PT做种的是一个btrfs文件系统，原本备份home,root是zfs的dataset。

解决办法是先设置nfs共享数据，vm将其挂载到本地
#### setup

server
```

```

```
vim /etc/export

/mnt/Disk2      *(rw,sync,no_subtree_check,no_root_squash)
/mnt/Archive    *(rw,sync,no_subtree_check,no_root_squash)
```

```
exportfs -a
```

client
```
mount hostname:/path/  /mnt/path
```

#### 挂载后只读问题

解决办法为

`no_root_squash` 是NFS服务器的一个选项，它控制着NFS共享中对root用户的访问权限。具体来说，它的含义如下：
1. **Root Squashing（根用户压制）**：在NFS共享中，默认情况下，如果一个远程客户端尝试使用root用户权限进行写入操作，NFS服务器会将其权限压制（squash），将其映射为一个非特权用户，以提高安全性。这意味着root用户的操作将被限制，防止对NFS服务器上的共享目录进行不受控制的写入。
2. **no_root_squash（不进行根用户压制）**：当设置了`no_root_squash`选项时，NFS服务器将允许远程客户端的root用户以其真实的root权限进行写入操作。这意味着root用户在NFS共享上具有与本地文件系统上相同的权限。这通常用于需要root权限来进行管理或配置的情况。

`no_root_squash` 选项的使用需要谨慎，因为它可能会降低NFS服务器的安全性。如果不是绝对需要，最好避免使用此选项，并使用其他方式来管理共享文件系统的权限和访问控制。在某些情况下，可以考虑将特定用户或客户端映射为非root用户，而不是禁用整个根用户压制。这样可以更精确地控制权限。
#### nfs fstab

```
192.168.35.254:/mnt/Disk2 /mnt/Disk2/ nfs rw,hard,noatime,rsize=8192,wsize=8192 0 0
192.168.35.254:/mnt/Archive /mnt/Archive/ nfs rw,hard,noatime,rsize=8192,wsize=8192 0 0
```

格式：`[远程服务器]:[共享目录] [本地挂载点] nfs [挂载选项] 0 0`
- `[远程服务器]`：这是远程NFS服务器的主机名或IP地址。例如，`nfs-server.example.com` 或 `192.168.1.100`。
- `[共享目录]`：这是您要挂载的远程NFS共享的路径。例如，`/mnt/nfs-share`。
- `[本地挂载点]`：这是本地系统上的目录，您希望将远程NFS共享挂载到该目录。例如，`/mnt/local-mount`。
- `nfs`：这是文件系统类型，用于指定要挂载的文件系统类型。
- `[挂载选项]`：这是一系列挂载选项，用于配置NFS挂载的行为。以下是一些常见的NFS挂载选项：
    - `rw`：允许读写访问。
    - `ro`：只允许只读访问。
    - `noatime`：不更新文件的访问时间戳（atime）。
    - `hard` 或 `soft`：指定NFS挂载的错误处理方式，`hard`表示重试，`soft`表示超时。
    - `rsize` 和 `wsize`：指定读取和写入数据块的大小，以影响性能。
    - `timeo` 和 `retrans`：设置超时和重传参数。
- `0` 和 `0`：这是备份检查和文件系统检查选项，通常将它们设置为0。

### root

备份内容
- ssh
- root
    - .acme.sh
- nginx
- crontab
```
/var/spool/cron/crontabs
```
### home

直接rsync即可
### docker

pve中将zvol作为vm单独一个盘
在vm中将其挂载即可

## 其它
### linux添加gpu

pve uefi关闭保护模式，否则nvidia驱动无法加载
  - 在启动vm时，esc进入bios，关闭保护模式

### jellyfin数据迁移


