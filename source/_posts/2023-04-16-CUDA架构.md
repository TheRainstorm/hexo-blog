---
title: 2023-04-16-CUDA架构
date: 2023-04-16 16:00
tags:
- cuda
categories:
- 学习
---

## 疑问

- ampere架构SM中有4个process block，process block对应一个warp？意思是可以有4个warp同时执行？
  - Femi架构没有process block，SM就是最小单元？
- The threads of a thread block execute concurrently on one multiprocessor, and multiple thread blocks can execute concurrently on one multiprocessor.
  - 这样岂不是若干thread block抢一个SM上的shared memory？
  - 不同threadblock的warp并发执行，如何隐藏延迟

- cuda分块大小对性能影响很大，那么如何确定分块大小呢？
  - 穷举
  - 分析模型？

<!-- more -->

# NV架构列表

## compute capacity

[Matching CUDA arch and CUDA gencode for various NVIDIA architectures - Arnon Shimoni](https://arnon.dk/matching-sm-architectures-arch-and-gencode-for-various-nvidia-cards/)

- Pascal
  - SM61: 1080
- volta
  - SM70: Tesla V100, Titan V
- Ampere
  - SM80: NVIDIA A100 (the name “Tesla” has been dropped – GA100))
![](https://raw.githubusercontent.com/TheRainstorm/.image-bed/main/20230419195715.png)


- Tesla(2008)
- Fermi(2010)
- Kepler(2012)：**K80**
- Maxwell(2014)： M10/M40
- Pascal(2016)： Tesla **P40**、**P100**、GTX **1080Ti**  Titan XP、Quadro GP100/P6000/P5000，10系
- Volta(2017)： Tesla **V100**、GeForce **Titan V**、Quadro GV100专业卡
- Turing(2018)： 1个SM 8个Tensor core，1个RT core，16，20系
- Ampere(2020)： **A100**，30系
- Hopper(2022)：H100

1080: 20x128
1080ti: 28x128, gp104
p40: 30x128, gp102
p100: 28x128, HBM2(4096bit)

[Ampere (microarchitecture) - Wikipedia](https://en.wikipedia.org/wiki/Ampere_(microarchitecture))

![](https://raw.githubusercontent.com/TheRainstorm/.image-bed/main/picgo/20230404165003.png)

[NVIDIA GPU 架构梳理 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/394352476)

### Fermi

-   橙色部分：2 个 Warp Scheduler/Dispatch Unit
-   绿色部分：32 个 CUDA 内核，分在两条 lane 上，每条分别是 16 个

https://stackoverflow.com/a/10467342
> The programmer divides work into threads, threads into thread blocks, and thread blocks into grids. The compute work distributor allocates thread blocks to Streaming Multiprocessors (SMs). **Once a thread block is distributed to a SM the resources for the thread block are allocated** (warps and shared memory) and threads are divided into groups of 32 threads called warps. Once a warp is allocated it is called an active warp. **The two warp schedulers pick two active warps per cycle** and dispatch warps to execution units. For more details on execution units and instruction dispatch see [1](http://www.nvidia.com/content/PDF/fermi_white_papers/NVIDIA_Fermi_Compute_Architecture_Whitepaper.pdf) p.7-10 and [2](http://www.anandtech.com/show/3809/nvidias-geforce-gtx-460-the-200-king/2).

Fermi，一个SM有两个warp保证每周期有指令可以发射
> A stalled warp is ineligible to be selected by the warp scheduler. On Fermi it is useful to have at least 2 eligible warps per cycle so that the warp scheduler can issue an instruction.

GeForce 560Ti，8SM，每个48CUDA
>  If you launch kernel<<<8, 48>>> you will get 8 blocks each with 2 warps of 32 and 16 threads. There is no guarantee that these 8 blocks will be assigned to different SMs.

- 每个SM可以有很多线程块
> A GTX560 can have 8 SM * 8 blocks = 64 blocks at a time or 8 SM * 48 warps = 512 warps if the kernel does not max out registers or shared memory. At any given time on a portion of the work will be active on SMs. Each SM has multiple execution units (more than CUDA cores). Which resources are in use at any given time is dependent on the warp schedulers and instruction mix of the application. If you don't do TEX operations then the TEX units will be idle. If you don't do a special floating point operation the SUFU units will idle.

![](https://pic2.zhimg.com/80/v2-5aaf90a4f9cb41af90833a978d735c89_720w.webp)

#### 白皮书
[Microsoft Word - NVIDIA Fermi Architecture Whitepaper.docx](https://www.nvidia.com/content/PDF/fermi_white_papers/NVIDIA_Fermi_Compute_Architecture_Whitepaper.pdf)

双发射
![](https://raw.githubusercontent.com/TheRainstorm/.image-bed/main/picgo/20230414193634.png)

> Fermi’s dual warp scheduler selects two warps, and issues one instruction from each warp to a group of sixteen cores, sixteen load/store units, or four SFUs. Because warps execute independently, Fermi’s scheduler does not need to check for dependencies from within the instruction stream. Using this elegant model of dual-issue, Fermi achieves near peak hardware performance.
> Most instructions can be dual issued; two integer instructions, two floating instructions, or a mix of integer, floating point, load, store, and SFU instructions can be issued concurrently. Double precision instructions do not support dual dispatch with any other operation.

可配置的shared memory和L1 cache
> G80 and GT200 have 16 KB of shared memory per SM. In the Fermi architecture, each SM has 64 KB of on-chip memory that can be configured as 48 KB of Shared memory with 16 KB of L1 cache or as 16 KB of Shared memory with 48 KB of L1 cache.

### Kepler

- 一个SMX 192个CUDA
![](https://pic3.zhimg.com/80/v2-8130651bd394205a5f9fb9c736085b96_720w.webp)

### Maxwell

- SMM：四个处理块(processing block)，每个有专用的warp调度器，包含32个core
![](https://pic3.zhimg.com/80/v2-3cd6ea7b8bfd5830760e022393da0b1a_720w.webp)

### volta
跳过了pascal：一个SM两个处理块

- SM：4个block
- 将一个CUDA拆分成FP32和INT32，每个周期可以同时执行浮点和整数。
- 添加tensor core
![](https://pic4.zhimg.com/80/v2-ab5cc1ac8a897332cdb9d6565cf9c7af_720w.webp)

### ampere架构

跳过turing：去掉了F64

![](https://pic2.zhimg.com/80/v2-ab9a493303f4902b1dace22df0fb652d_720w.webp)

# CUDA架构

## 参考资料

- [CUDA C++ Programming Guide (nvidia.com)](https://docs.nvidia.com/cuda/cuda-c-programming-guide/)

## 基础

In CUDA programming, both CPUs and GPUs are used for computing. Typically, we refer to CPU and GPU system as _host_ and _device_, respectively. CPUs and GPUs are separated platforms with their own memory space. Typically, we run serial workload on CPU and offload parallel computation to GPUs.

- 三个关键抽象：
  - 层次化的线程组
  - 共享内存
  - 同步

高度可扩展性：
![](https://raw.githubusercontent.com/TheRainstorm/.image-bed/main/picgo/20230404193556.png)

## 编程模型

### 线程层次

kernel
- C++函数，被调用时，会被每个CUDA线程并行执行。
- 使用__global__声明kernel函数
- 使用`<<<...>>>`_execution configuration_ syntax，指定使用多少线程执行该kernel

线程层次
- block：多个线程组成一个线程块。可以是1,2,3维的
  - 通过`threadIdx`索引。如二维 _(x, y)_ 的线程id是 _(x + y Dx)_;
  - 一个块内的线程数是有限制的（当前的GPU一般为1024个）。因为一个块内的线程会被调度到一个SM(streaming multiprocessor core)中，共享该SM的片上存储（shared memory）
  - 一个块是独立的，可以以任意顺序调度，从而保证了GPU的可扩展性（SM是基本单元，堆SM）
  - shared memory延迟很低，类似于L1 cache
- grid：多个线程块组成一个grid。可以是1,2,3维的
  - 通过`blockIdx`索引

以下代码声明了1个线程块，大小为NxN。用于将两个NxN的矩阵相加。
```c
// Kernel definition
__global__ void MatAdd(float A[N][N], float B[N][N],
                       float C[N][N])
{
    int i = threadIdx.x;
    int j = threadIdx.y;
    C[i][j] = A[i][j] + B[i][j];
}

int main()
{
    ...
    // Kernel invocation with one block of N * N * 1 threads
    int numBlocks = 1;
    dim3 threadsPerBlock(N, N);
    MatAdd<<<numBlocks, threadsPerBlock>>>(A, B, C);
    ...
}
```

以下代码声明了N/16 x N/16个线程块，每个大小为16x16。用于将两个NxN的矩阵相加。
```c
// Kernel definition
__global__ void MatAdd(float A[N][N], float B[N][N],
float C[N][N])
{
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    int j = blockIdx.y * blockDim.y + threadIdx.y;
    if (i < N && j < N)
        C[i][j] = A[i][j] + B[i][j];
}

int main()
{
    ...
    // Kernel invocation
    dim3 threadsPerBlock(16, 16);
    dim3 numBlocks(N / threadsPerBlock.x, N / threadsPerBlock.y);
    MatAdd<<<numBlocks, threadsPerBlock>>>(A, B, C);
    ...
}
```

![](https://raw.githubusercontent.com/TheRainstorm/.image-bed/main/picgo/20230404195709.png)

- 在线程块内，线程通过shared memory来共享数据。并且通过同步操作来协调内存访问
  - `__syncthreads()`用于路障同步


- 线程块蔟：
  - CUDA 9.0中引入的一个可选层次
  - 类似于线程块内线程保证在同一个SM。一个cluster内的线程块被调度到同一个GPU Processing Cluster (GPC)
  - 大小一般最大8个块
  - 支持硬件支持的同步api。cluster.sync()
![](https://raw.githubusercontent.com/TheRainstorm/.image-bed/main/picgo/20230404200253.png)


### 编程接口

- 包含对C++的少量扩展和rutime库

C++扩展
- 定义kernel
- 指定线程数

CUDA runtime
- 执行在host上
- 分配回收device内存
- 在host和device间传输数据
- 管理多个device

编译
- 将device代码编译成ptx或cubin
- 将host代码编译，和runtime链接
  - runtime基于底层另一层抽象层，该抽象层再基于driver API。

兼容性
- cubin只能在小版本里后向兼容。_cubin_ object generated for compute capability _X.y_ will only execute on devices of compute capability _X.z_ where _z≥y_.
- PTX可以后向兼容，但是无法利用新硬件特性。a binary targeting devices of compute capability 7.0 (Volta) compiled from PTX generated for compute capability 6.0 (Pascal) will not make use of Tensor Core instructions, since these were not available on Pascal.
- 后向兼容(backward)：旧编译的可以在新平台上运行

![](https://raw.githubusercontent.com/TheRainstorm/.image-bed/main/picgo/20230406141727.png)

## 硬件实现

https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#hardware-implementation

> The NVIDIA GPU architecture is built around a scalable array of multithreaded _Streaming Multiprocessors_ (_SMs_). When a CUDA program on the host CPU invokes a kernel grid, the blocks of the grid are enumerated and distributed to multiprocessors with available execution capacity. The threads of a thread block execute concurrently on one multiprocessor, and multiple thread blocks can execute concurrently on one multiprocessor. As thread blocks terminate, new blocks are launched on the vacated multiprocessors.

- 英伟达GPU架构由SM数组组成，具有可扩展性。
- 当host上的一个CUDA程序调用一个kernel grid时，grid中的线程块被分发到有计算能力的SM上执行。
- 一个线程块内的线程在一个multiprocessor内并发执行，并且多个线程块也可以并发调度到一个SM上（当一个线程块终止时，新的块补上） *p.s 这里说的是并发，可能需要和并行区分*

- SM被设计来并发执行上百个线程，采用了SIMT架构（_Single-Instruction, Multiple-Thread_）
  - 单线程内利用流水线实现ILP
  - 通过同时多线程（simultaneous hardware multithreading）实现**线程级并行**
    - 和CPU的SMT不同。Unlike CPU cores, they are issued in order and there is no branch prediction or speculative execution.

### SIMT

自己的理解：CPU一个核可以并发执行两个线程（超线程技术），而GPU一个SM可以并发运行成百上千的线程。为了做到这点，采用了SIMT技术

- warp
- warp内的线程执行

- SM以一个warp 32个线程为单位，进行调度。The multiprocessor creates, manages, schedules, and executes threads in groups of 32 parallel threads called _warps_. 
- 每个warp的线程从相同的PC开始执行。但是它们**内部有自己的PC可以单独跳转**。Individual threads composing a warp start together at the same program address, but they have their own instruction address counter and register state and are therefore free to branch and execute independently.
- **SM将线程块划分为warp进行调度**。When a multiprocessor is given one or more thread blocks to execute, it partitions them into warps and each warp gets scheduled by a _warp scheduler_ for execution
- 线程块的划分很简单，连续的线程被划分在一起。The way a block is partitioned into warps is always the same; each warp contains threads of consecutive, increasing thread IDs with the first warp containing thread 0.
- warp中的线程从相同地址开始执行，如果线程因为数据相关的分支造成分叉，**warp执行每一条代码路径**，同时禁止非该代码路径上的线程。A warp executes one common instruction at a time. ... If threads of a warp diverge via a data-dependent conditional branch, the warp executes each branch path taken, disabling threads that are not on that path.
- **分叉只发生在warp内**，不同warp是独立的。Branch divergence occurs only within a warp; different warps execute independently regardless of whether they are executing common or disjoint code paths.

- **GPU的SIMT和SIMD有点类似，都是单条指令处理多个数据**。The SIMT architecture is akin to SIMD (Single Instruction, Multiple Data) vector organizations in that a single instruction controls multiple processing elements.
  - 关键的不同在于，SIMT既可以实现线程级并行（对于独立的标量线程），又可以实现数据级并行（对于合作线程）。SIMT enables programmers to write thread-level parallel code for independent, scalar threads, as well as data-parallel code for coordinated threads.

- volta之前的架构，warp内32个线程公用相同的PC。导致分叉路径上的线程无法相互通信。Prior to NVIDIA Volta, warps used a single program counter shared amongst all 32 threads in the warp together with an active mask specifying the active threads of the warp. As a result, threads from the same warp in divergent regions or different states of execution cannot signal each other or exchange data, and algorithms requiring fine-grained sharing of data guarded by locks or mutexes can easily lead to deadlock, depending on which warp the contending threads come from.

### Hardware Multithreading

- 执行上下文包含PC，寄存器。warp上下文被保存在片上（而不是软件保存），因此warp切换没有损失。The execution context (program counters, registers, and so on) for each warp processed by a multiprocessor is maintained on-chip during the entire lifetime of the warp. Therefore, switching from one execution context to another has no cost, 

## Shared memory or cache ?

[Is it possible to use L1 cache instead of shared memory when implementing blocked matmuls in CUDA - CUDA / CUDA Programming and Performance - NVIDIA Developer Forums](https://forums.developer.nvidia.com/t/is-it-possible-to-use-l1-cache-instead-of-shared-memory-when-implementing-blocked-matmuls-in-cuda/256985/3)

起初没有L1/L2 --> 引入scratch pad --> cache越来越大
> I think it is fair to say that the importance of shared memory in CUDA programming has decreased with the advent of L1/L2 caches of competitive size in GPUs. For use cases requiring peak performance, shared memory can still be important due to the programmer control it provides.


## tensor core

v100 whitepaper
> The Volta tensor cores are accessible and exposed as Warp-Level Matrix Operations in the CUDA 9 C++ API. The API exposes specialized matrix load, matrix multiply and accumulate, and matrix store operations to efficiently use Tensor Cores from a CUDA-C++ program. At the CUDA level, the warp-level interface assumes 16x16 size matrices spanning all 32 threads of the warp. In addition to CUDA-C++ interfaces to program Tensor Cores directly, cuBLAS and cuDNN libraries have been updated to provide new library interfaces to make use of Tensor Cores for deep learning applications and frameworks. NVIDIA has worked with many popular deep learning frameworks such as Caffe2 and MXNet to enable use of Tensor Cores for deep learning research on Volta GPU based systems. NVIDIA is working to add support for Tensor Cores in other frameworks as well.